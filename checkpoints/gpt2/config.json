{
    "gpt_variant": "gpt2",
    "bias": true,
    "q_scaling": 1,
    "embedding_scale": null,
    "apply_query_key_layer_scaling": false,
    "rotary_pct": 1.0,
    "rotary_base": 10000.0,
    "rotary_scaling": null,
    "inner_layernorm": false,
    "norm_before_bmm1": false,
    "moe": {
        "num_experts": 0,
        "shared_expert_intermediate_size": 0,
        "top_k": 0,
        "normalization_mode": 1,
        "sparse_mixer_epsilon": 0.01,
        "tp_mode": 0,
        "device_limited_n_group": 0,
        "device_limited_topk_group": 0,
        "device_limited_routed_scaling_factor": 1.0
    },
    "architecture": "GPT2LMHeadModel",
    "dtype": "float16",
    "vocab_size": 50257,
    "hidden_size": 1024,
    "num_hidden_layers": 24,
    "num_attention_heads": 16,
    "hidden_act": "gelu_new",
    "logits_dtype": "float32",
    "norm_epsilon": 1e-05,
    "runtime_defaults": null,
    "position_embedding_type": "learned_absolute",
    "num_key_value_heads": 16,
    "intermediate_size": 4096,
    "max_position_embeddings": 1024,
    "mapping": {
        "world_size": 1,
        "gpus_per_node": 8,
        "cp_size": 1,
        "tp_size": 1,
        "pp_size": 1,
        "moe_tp_size": 1,
        "moe_ep_size": 1,
        "auto_parallel": false
    },
    "quantization": {
        "quant_algo": null,
        "kv_cache_quant_algo": null,
        "group_size": 128,
        "smoothquant_val": 0.5,
        "clamp_val": null,
        "use_meta_recipe": false,
        "has_zero_point": false,
        "pre_quant_scale": false,
        "exclude_modules": null
    },
    "use_parallel_embedding": false,
    "embedding_sharding_dim": 0,
    "head_size": 64,
    "qk_layernorm": false,
    "rotary_embedding_dim": 64
}